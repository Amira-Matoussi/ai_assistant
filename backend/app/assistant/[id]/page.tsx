"use client"

import { useState, useRef, useCallback, useEffect } from "react"
import { Button } from "@/components/ui/button"
import { Card, CardContent } from "@/components/ui/card"
import { Volume2, AlertTriangle, History, ArrowLeft, Mic, MicOff, Square, Play, MessageSquare, AudioLines, User, Download, PlayCircle } from "lucide-react"
import { useToast } from "@/hooks/use-toast"
import { Avatar } from "@/components/avatar"
import Link from "next/link"
import { useParams, useRouter } from "next/navigation"
import Image from "next/image"
import { useLanguage } from "@/hooks/use-language"
import { getStatus } from "@/utils/status"

interface ConversationTurn {
  user: string
  ai: string
  timestamp: Date
  user_audio_path?: string
  ai_audio_path?: string
}

interface AssistantConfig {
  id: number
  name: string
  gender: "Male" | "Female"
  type: string
  image: string
  systemPrompt: string
  features: string[]
  color: string
  bgColor: string
}

type AssistantName = "Slah" | "Amira";
type LanguageCode = "en-US" | "fr-FR" | "ar-SA";

const VOICE_CONFIG: Record<AssistantName, Record<LanguageCode, string>> = {
  "Slah": {
    "en-US": "pNInz6obpgDQGcFmaJgB",
    "fr-FR": "pNInz6obpgDQGcFmaJgB",
    "ar-SA": "Qp2PG6sgef1EHtrNQKnf",
  },
  "Amira": {
    "en-US": "21m00Tcm4TlvDq8ikWAM",
    "fr-FR": "21m00Tcm4TlvDq8ikWAM",
    "ar-SA": "4wf10lgibMnboGJGCLrP",  
  }
}

export default function VoiceAssistant() {
  const params = useParams()
  const router = useRouter()
  const assistantId = Number.parseInt(params.id as string)
  const { language, t, isRTL } = useLanguage()

  // Authentication states
  const [userId, setUserId] = useState<string | null>(null)
  const [userEmail, setUserEmail] = useState<string | null>(null)
  const [authToken, setAuthToken] = useState<string | null>(null)

  const [isRecording, setIsRecording] = useState(false)
  const [isProcessing, setIsProcessing] = useState(false)
  const [transcription, setTranscription] = useState("")
  const [aiResponse, setAiResponse] = useState("")
  const [isPlaying, setIsPlaying] = useState(false)
  const [speechRecognitionSupported, setSpeechRecognitionSupported] = useState(false)
  
  // Enhanced conversation state management
  const [conversationHistory, setConversationHistory] = useState<ConversationTurn[]>([])
  const [currentSessionId, setCurrentSessionId] = useState<string>("")
  const [isNewSession, setIsNewSession] = useState(true)
  
  // Manual control states
  const [autoMode, setAutoMode] = useState(false)
  const [currentAudio, setCurrentAudio] = useState<HTMLAudioElement | null>(null)
  const [playingAudio, setPlayingAudio] = useState<string | null>(null)
  
  // Audio recording states
  const mediaRecorderRef = useRef<MediaRecorder | null>(null)
  const audioChunksRef = useRef<Blob[]>([])
  const streamRef = useRef<MediaStream | null>(null)
  const recognitionRef = useRef<any>(null)
  const finalTranscriptRef = useRef<string>("")
  const { toast } = useToast()

  // Check authentication on component mount
  useEffect(() => {
    const token = localStorage.getItem("authToken")
    const storedUserId = localStorage.getItem("userId")
    const storedEmail = localStorage.getItem("userEmail")
    
    if (token && storedUserId) {
      setAuthToken(token)
      setUserId(storedUserId)
      setUserEmail(storedEmail)
    }
  }, [])

  // Enhanced session initialization with conversation loading
  useEffect(() => {
    const initializeSession = async () => {
      if (!currentSessionId) {
        const newSessionId = crypto.randomUUID()
        setCurrentSessionId(newSessionId)
        console.log("üÜï Created new session:", newSessionId)
        setIsNewSession(true)
      } else {
        // Always try to load existing conversation history
        console.log("üìö Checking for existing conversation history...")
        await loadConversationHistory()
      }
    }
    
    initializeSession()
  }, [currentSessionId])

  // Assistant configurations
  const assistantConfigs: Record<number, AssistantConfig> = {
    1: {
      id: 1,
      name: "Slah",
      gender: "Male",
      type: t("assistant.slah.type"),
      image: "/images/slah.png",
      systemPrompt: "",
      features: [],
      color: "from-blue-200 to-blue-300",
      bgColor: "bg-blue-25",
    },
    2: {
      id: 2,
      name: "Amira", 
      gender: "Female",
      type: t("assistant.amira.type"),
      image: "/images/amira.png",
      systemPrompt: "",
      features: [],
      color: "from-pink-200 to-pink-300",
      bgColor: "bg-pink-25",
    },
  }

  const currentAssistant = assistantConfigs[assistantId] || assistantConfigs[1]

  // Enhanced loadConversationHistory function
  const loadConversationHistory = async () => {
    if (!currentSessionId) {
      console.log("‚ö†Ô∏è Cannot load history: missing session ID")
      return
    }
    
    try {
      console.log(`üìö Loading conversation history for session: ${currentSessionId}`)
      
      const headers: any = {}
      if (authToken) {
        headers["Authorization"] = `Bearer ${authToken}`
      }
      
      const response = await fetch(`/api/conversation-history/${currentSessionId}`, {
        headers: headers
      })
      
      if (response.ok) {
        const history = await response.json()
        console.log("üìö Loaded conversation history:", history.length, "items")
        
        if (history.length > 0) {
          const formattedHistory = history.map((item: any) => ({
            user: item.user_message,
            ai: item.ai_response,
            timestamp: new Date(item.timestamp),
            user_audio_path: item.user_audio_path,
            ai_audio_path: item.ai_audio_path
          }))
          
          setConversationHistory(formattedHistory)
          setIsNewSession(false)
          
          console.log(`Restored ${history.length} previous messages`)
        } else {
          console.log("üìù No existing conversation found for this session")
          setIsNewSession(true)
        }
      } else if (response.status === 404) {
        console.log("üìù Session doesn't exist yet")
        setIsNewSession(true)
      } else {
        console.error("Failed to load conversation history:", response.status)
      }
    } catch (error) {
      console.error("Failed to load conversation history:", error)
    }
  }

  // Initialize audio recording
  const initializeAudioRecording = useCallback(async (): Promise<boolean> => {
    try {
      console.log("üé§ Initializing audio recording...")
      
      if (mediaRecorderRef.current && mediaRecorderRef.current.state !== 'inactive') {
        mediaRecorderRef.current.stop()
        mediaRecorderRef.current = null
      }
      if (streamRef.current) {
        streamRef.current.getTracks().forEach(track => track.stop())
        streamRef.current = null
      }

      const stream = await navigator.mediaDevices.getUserMedia({ 
        audio: {
          echoCancellation: true,
          noiseSuppression: true,
          sampleRate: 44100,
          channelCount: 1
        }
      })
      streamRef.current = stream
      
      audioChunksRef.current = []
      
      const mimeTypes = [
        'audio/webm;codecs=opus',
        'audio/webm', 
        'audio/mp4',
        'audio/ogg'
      ]
      
      let selectedMimeType = ''
      for (const mimeType of mimeTypes) {
        if (MediaRecorder.isTypeSupported(mimeType)) {
          selectedMimeType = mimeType
          break
        }
      }
      
      if (!selectedMimeType) {
        console.error("‚ùå No supported audio MIME types found")
        return false
      }
      
      const mediaRecorder = new MediaRecorder(stream, { mimeType: selectedMimeType })
      
      mediaRecorder.ondataavailable = (event) => {
        if (event.data.size > 0) {
          audioChunksRef.current.push(event.data)
        }
      }
      
      mediaRecorder.onstop = () => {
        const totalSize = audioChunksRef.current.reduce((sum, chunk) => sum + chunk.size, 0)
        console.log(`üìπ Total audio size: ${totalSize} bytes`)
      }

      mediaRecorder.onerror = (event) => {
        console.error("üìπ MediaRecorder error:", event)
      }
      
      mediaRecorderRef.current = mediaRecorder
      mediaRecorder.start(250)
      
      return true
    } catch (error) {
      console.error("‚ùå Failed to initialize audio recording:", error)
      toast({
        title: "Audio Recording Failed",
        description: "Could not access microphone. Please check permissions.",
        variant: "destructive"
      })
      return false
    }
  }, [toast])

  // Stop audio recording and return blob
  const stopAudioRecording = useCallback((): Blob | null => {
    if (mediaRecorderRef.current && mediaRecorderRef.current.state === 'recording') {
      mediaRecorderRef.current.stop()
    }
    
    if (streamRef.current) {
      streamRef.current.getTracks().forEach(track => track.stop())
      streamRef.current = null
    }

    if (audioChunksRef.current.length > 0) {
      const mimeType = mediaRecorderRef.current?.mimeType || 'audio/webm'
      const audioBlob = new Blob(audioChunksRef.current, { type: mimeType })
      audioChunksRef.current = []
      return audioBlob
    }
    return null
  }, [])

  // Convert audio blob to base64
  const blobToBase64 = (blob: Blob): Promise<string> => {
    return new Promise((resolve, reject) => {
      const reader = new FileReader()
      reader.onloadend = () => {
        const result = reader.result as string
        console.log(`üîÑ FileReader result length: ${result.length}`)
        console.log(`üîÑ FileReader result start: ${result.substring(0, 50)}...`)
        resolve(result) // Return the full data URL with prefix
      }
      reader.onerror = (error) => {
        console.error("üîÑ FileReader error:", error)
        reject(error)
      }
      reader.readAsDataURL(blob)
    })
  }

  // Initialize speech recognition
  const initializeSpeechRecognition = useCallback(() => {
    if (!speechRecognitionSupported) return null

    try {
      const SpeechRecognition = (window as any).SpeechRecognition || (window as any).webkitSpeechRecognition
      const recognition = new SpeechRecognition()

      recognition.continuous = false
      recognition.interimResults = false
      recognition.lang = language === "ar-SA" ? "ar-EG" : language
      recognition.maxAlternatives = 1

      recognition.onstart = async () => {
        console.log("üé§ Speech recognition started")
        setIsRecording(true)
        setTranscription("")
        finalTranscriptRef.current = ""
        
        const success = await initializeAudioRecording()
        if (!success) {
          console.error("Failed to start audio recording")
        }
      }

      recognition.onresult = (event: any) => {
        const transcript = event.results[0][0].transcript
        console.log("üé§ Speech recognition result:", transcript)
        setTranscription(transcript)
        finalTranscriptRef.current = transcript
      }

      recognition.onend = () => {
        console.log("üé§ Speech recognition ended")
        setIsRecording(false)
        
        const audioBlob = stopAudioRecording()
        const finalTranscript = finalTranscriptRef.current
        
        if (finalTranscript) {
          processTranscription(finalTranscript, audioBlob)
        }
        finalTranscriptRef.current = ""
      }

      recognition.onerror = (event: any) => {
        console.error("üé§ Speech recognition error:", event.error)
        setIsRecording(false)
        setIsProcessing(false)
        setIsPlaying(false)
        finalTranscriptRef.current = ""
        stopAudioRecording()

        switch (event.error) {
          case "no-speech":
            console.log("üëÇ No speech detected")
            break
          case "audio-capture":
            toast({
              title: "Microphone Issue",
              description: "Please check microphone permissions.",
              variant: "destructive",
            })
            break
          case "not-allowed":
            toast({
              title: "Permission Required", 
              description: "Please allow microphone access.",
              variant: "destructive",
            })
            break
          default:
            toast({
              title: "Recognition Issue",
              description: "Please try again.",
              variant: "default",
            })
        }
      }

      return recognition
    } catch (error) {
      console.error("Failed to initialize speech recognition:", error)
      return null
    }
  }, [speechRecognitionSupported, language, toast, initializeAudioRecording, stopAudioRecording])

  // Start recording
  const startRecording = useCallback(() => {
    const recognition = initializeSpeechRecognition()
    if (recognition) {
      recognitionRef.current = recognition
      recognition.start()
    }
  }, [initializeSpeechRecognition])

  // Stop recording
  const stopRecording = useCallback(() => {
    if (recognitionRef.current && isRecording) {
      recognitionRef.current.stop()
    }
  }, [isRecording])

  // Enhanced processTranscription function with dual audio support
  const processTranscription = async (transcript: string, audioBlob: Blob | null = null) => {
    setIsProcessing(true)
    try {
      console.log(`üì§ Processing transcription: "${transcript}"`)
      console.log(`üì§ Using session: ${currentSessionId}`)
      console.log(`üìö Current history length: ${conversationHistory.length}`)
      
      // Get audio data if available
      let audioData = null
      if (audioBlob && audioBlob.size > 0) {
        try {
          console.log(`üîä Converting audio blob to base64 - Size: ${audioBlob.size} bytes, Type: ${audioBlob.type}`)
          audioData = await blobToBase64(audioBlob)
          console.log(`‚úÖ Audio converted - Base64 length: ${audioData.length}`)
          
          // Verify the base64 data looks correct
          if (audioData.startsWith('data:audio')) {
            console.log(`‚úÖ Audio data format looks correct: ${audioData.substring(0, 50)}...`)
          } else {
            console.log(`‚ö†Ô∏è Audio data format may be incorrect: ${audioData.substring(0, 50)}...`)
          }
        } catch (error) {
          console.error("üìπ Failed to convert audio to base64:", error)
          audioData = null
        }
      } else {
        console.log("‚ö†Ô∏è No audio blob provided or blob is empty")
      }

      // Use authenticated endpoint if logged in
      const endpoint = authToken ? "/api/voice-pipeline-auth" : "/api/voice-pipeline"
      const headers: any = { "Content-Type": "application/json" }
      
      if (authToken) {
        headers["Authorization"] = `Bearer ${authToken}`
      }

      // Send current conversation history to maintain context
      const requestBody = {
        transcription: transcript,
        history: conversationHistory.map((c) => ({ user: c.user, ai: c.ai })),
        language: language,
        sessionId: currentSessionId,
        assistantId: assistantId,
        audioData: audioData, // Include the base64 audio data
      }

      console.log(`üì§ Sending request to ${endpoint}`)
      console.log(`üì§ Request body - Audio included: ${!!audioData}`)

      const response = await fetch(endpoint, {
        method: "POST",
        headers: headers,
        body: JSON.stringify(requestBody),
      })

      if (!response.ok) {
        throw new Error(`HTTP error! status: ${response.status}`)
      }

      const result = await response.json()
      console.log("üì§ Response received:", result)
      console.log("üì§ Audio was saved:", result.audioSaved)
      
      setAiResponse(result.aiResponse)
      
      // Use the complete conversation history returned by the backend
      if (result.conversationHistory && Array.isArray(result.conversationHistory)) {
        console.log("üîÑ Updating with complete backend history:", result.conversationHistory.length, "items")
        
        // Convert backend format to frontend format with dual audio support
        const backendHistory = result.conversationHistory.map((item: any) => ({
          user: item.user || item.user_message,
          ai: item.ai || item.ai_response,
          timestamp: new Date(item.timestamp || Date.now()),
          user_audio_path: item.user_audio_path,
          ai_audio_path: item.ai_audio_path
        }))
        
        setConversationHistory(backendHistory)
      } else {
        // Fallback: add current turn to existing history
        const newTurn: ConversationTurn = {
          user: transcript,
          ai: result.aiResponse,
          timestamp: new Date(),
          user_audio_path: result.audioSaved?.user ? result.audioSaved.user : undefined,
          ai_audio_path: result.audioSaved?.ai ? result.audioSaved.ai : undefined
        }
        
        setConversationHistory((prev) => [...prev, newTurn])
      }
      
      setIsNewSession(false)
      setIsProcessing(false)

      // Play response using enhanced speakText function
      await speakText(result.aiResponse)

    } catch (error) {
      console.error("‚ùå Error processing transcription:", error)
      toast({
        title: "Processing Failed",
        description: error instanceof Error ? error.message : "Please try again",
        variant: "destructive",
      })
      setIsProcessing(false)
    }
  }

  // Enhanced startNewConversation function
  const startNewConversation = () => {
    const newSessionId = crypto.randomUUID()
    setCurrentSessionId(newSessionId)
    setConversationHistory([]) // Clear existing history
    setTranscription("")
    setAiResponse("")
    setIsNewSession(true)
    
    // Stop any current audio
    if (currentAudio) {
      currentAudio.pause()
      setCurrentAudio(null)
      setIsPlaying(false)
      setPlayingAudio(null)
    }
    
    console.log("üÜï Started new conversation:", newSessionId)
    
    toast({
      title: "New Conversation Started",
      description: "Previous conversation history cleared",
      variant: "default",
    })
  }

  // Enhanced speakText function with pre-generated AI audio support
  const speakText = async (text: string) => {
    return new Promise<void>((resolve) => {
      const playAudio = async () => {
        try {
          const assistantName = currentAssistant.name as AssistantName
          const langCode = language as LanguageCode
          const voiceId = VOICE_CONFIG[assistantName]?.[langCode] || VOICE_CONFIG["Slah"]["en-US"]
          
          // Check if we already have AI audio for this message
          const currentTurn = conversationHistory[conversationHistory.length - 1]
          if (currentTurn && currentTurn.ai_audio_path && currentTurn.ai === text) {
            // Use pre-generated audio from backend
            console.log("üîä Using pre-generated AI audio:", currentTurn.ai_audio_path)
            const audioUrl = `http://localhost:8000/recordings/${currentTurn.ai_audio_path}`
            const audio = new Audio(audioUrl)
            
            setCurrentAudio(audio)
            setIsPlaying(true)
            setPlayingAudio(currentTurn.ai_audio_path)
            
            audio.onended = () => {
              setIsPlaying(false)
              setCurrentAudio(null)
              setPlayingAudio(null)
              resolve()
              
              if (autoMode) {
                setTimeout(() => {
                  startRecording()
                }, 500)
              }
            }
            
            audio.onerror = () => {
              console.warn("Pre-generated audio failed, falling back to TTS")
              // Fallback to real-time TTS
              fallbackToTTS()
            }
            
            await audio.play()
            return
          }
          
          // Fallback to real-time TTS if no pre-generated audio
          fallbackToTTS()
          
          async function fallbackToTTS() {
            console.log("üîä Using real-time TTS")
            const response = await fetch("/api/elevenlabs-tts", {
              method: "POST",
              headers: { "Content-Type": "application/json" },
              body: JSON.stringify({
                text: text,
                voice_id: voiceId,
                language: language
              })
            })

            if (!response.ok) {
              throw new Error(`TTS failed: ${response.statusText}`)
            }

            const audioBlob = await response.blob()
            const audioUrl = URL.createObjectURL(audioBlob)
            const audio = new Audio(audioUrl)
            
            setCurrentAudio(audio)
            setIsPlaying(true)
            setPlayingAudio("real-time-tts")
            
            audio.onended = () => {
              setIsPlaying(false)
              setCurrentAudio(null)
              setPlayingAudio(null)
              URL.revokeObjectURL(audioUrl)
              resolve()
              
              if (autoMode) {
                setTimeout(() => {
                  startRecording()
                }, 500)
              }
            }
            
            audio.onerror = () => {
              setIsPlaying(false)
              setCurrentAudio(null)
              setPlayingAudio(null)
              URL.revokeObjectURL(audioUrl)
              resolve()
            }
            
            await audio.play()
          }
          
        } catch (error) {
          console.error("TTS error:", error)
          setIsPlaying(false)
          setCurrentAudio(null)
          setPlayingAudio(null)
          resolve()
        }
      }
      playAudio()
    })
  }

  // Stop audio
  const stopAudio = useCallback(() => {
    if (currentAudio) {
      currentAudio.pause()
      currentAudio.currentTime = 0
      setCurrentAudio(null)
      setIsPlaying(false)
      setPlayingAudio(null)
    }
  }, [currentAudio])

  // Play recording function for dual audio support
  const playRecording = (audioPath: string, audioType: 'user' | 'ai') => {
    if (currentAudio) {
      currentAudio.pause()
      setCurrentAudio(null)
      setIsPlaying(false)
      setPlayingAudio(null)
    }
    
    if (playingAudio === audioPath) {
      return
    }
    
    const audioUrl = `http://localhost:8000/recordings/${audioPath}`
    const audio = new Audio(audioUrl)
    setCurrentAudio(audio)
    setIsPlaying(true)
    setPlayingAudio(audioPath)
    
    audio.onended = () => {
      setIsPlaying(false)
      setCurrentAudio(null)
      setPlayingAudio(null)
    }
    
    audio.onerror = () => {
      console.error("Error playing recording")
      setIsPlaying(false)
      setCurrentAudio(null)
      setPlayingAudio(null)
      toast({
        title: "Playback Error",
        description: `Could not play ${audioType} audio recording`,
        variant: "destructive"
      })
    }
    
    audio.play()
  }

  // Download audio function
  const downloadAudio = (audioPath: string) => {
    const link = document.createElement('a')
    link.href = `http://localhost:8000/recordings/${audioPath}`
    link.download = audioPath
    document.body.appendChild(link)
    link.click()
    document.body.removeChild(link)
  }

  // Handle logout
  const handleLogout = () => {
    localStorage.clear()
    router.push("/auth/login")
  }

  // Check browser support
  useEffect(() => {
    const speechSupported = "webkitSpeechRecognition" in window || "SpeechRecognition" in window
    setSpeechRecognitionSupported(speechSupported)
  }, [])

  // Cleanup on unmount
  useEffect(() => {
    return () => {
      if (mediaRecorderRef.current && mediaRecorderRef.current.state === 'recording') {
        mediaRecorderRef.current.stop()
      }
      if (streamRef.current) {
        streamRef.current.getTracks().forEach(track => track.stop())
      }
    }
  }, [])

  return (
    <div className="min-h-screen bg-gradient-to-br from-indigo-50 via-white to-purple-50 pt-20">
      <div className="max-w-6xl mx-auto px-4 py-8">
        
        {/* Header with Controls */}
        <div className="flex items-center justify-between mb-8">
          <Link href="/assistant">
            <Button variant="outline" size="sm">
              <ArrowLeft className="w-4 h-4 mr-2" />
              {t("common.back")}
            </Button>
          </Link>
          
          {/* Controls */}
          <div className="flex items-center gap-4">
            <Button 
              onClick={startNewConversation}
              variant="outline" 
              size="sm"
              disabled={isRecording || isProcessing}
            >
              New Conversation
            </Button>
            
            <label className="flex items-center">
              <input
                type="checkbox"
                checked={autoMode}
                onChange={(e) => setAutoMode(e.target.checked)}
                className="mr-2"
              />
              Auto Mode
            </label>
          </div>
        </div>

        {/* Session Info */}
        <div className="text-center mb-4">
          <p className="text-sm text-gray-600">
            Session: {currentSessionId.slice(-8)}... ‚Ä¢ Messages: {conversationHistory.length}
            {playingAudio && (
              <span className="ml-2 text-blue-600">
                ‚Ä¢ Playing: {playingAudio === 'real-time-tts' ? 'Real-time TTS' : playingAudio}
              </span>
            )}
          </p>
        </div>

        {/* Manual Control Buttons */}
        <div className="flex justify-center mb-8">
          <div className="flex gap-4">
            {!isRecording ? (
              <Button
                onClick={startRecording}
                disabled={isProcessing || isPlaying}
                className="bg-red-500 hover:bg-red-600 text-white px-6 py-3"
              >
                <Mic className="w-5 h-5 mr-2" />
                Start Recording
              </Button>
            ) : (
              <Button
                onClick={stopRecording}
                className="bg-red-700 hover:bg-red-800 text-white px-6 py-3"
              >
                <MicOff className="w-5 h-5 mr-2" />
                Stop Recording
              </Button>
            )}

            {aiResponse && !isPlaying && (
              <Button
                onClick={() => speakText(aiResponse)}
                disabled={isRecording || isProcessing}
                className="bg-green-500 hover:bg-green-600 text-white px-6 py-3"
              >
                <Play className="w-5 h-5 mr-2" />
                Play Response
              </Button>
            )}

            {isPlaying && (
              <Button
                onClick={stopAudio}
                className="bg-red-500 hover:bg-red-600 text-white px-6 py-3"
              >
                <Square className="w-5 h-5 mr-2" />
                Stop Audio
              </Button>
            )}
          </div>
        </div>

        {/* Avatar */}
        <div className="flex justify-center mb-8">
          <Avatar
            assistantId={assistantId}
            isListening={isRecording}
            isProcessing={isProcessing}
            isSpeaking={isPlaying}
            hasConversationStarted={conversationHistory.length > 0}
            status={getStatus(isRecording, isProcessing, isPlaying, conversationHistory.length)}
            onToggleListening={() => {}}
            onInterrupt={() => {}}
          />
        </div>

        {/* Enhanced Full Conversation History Display with Dual Audio */}
        <div className="max-w-4xl mx-auto mb-8">
          <div className="bg-white/40 backdrop-blur-md rounded-2xl border border-white/20 shadow-xl p-6">
            <div className="flex items-center justify-between mb-4">
              <h3 className="text-lg font-semibold text-gray-800 flex items-center">
                <MessageSquare className="w-5 h-5 mr-2" />
                Conversation
              </h3>
              <div className="flex items-center space-x-2 text-sm text-gray-600">
                <span className="bg-blue-100 px-2 py-1 rounded-full">
                  {conversationHistory.length} messages
                </span>
                <span className="bg-gray-100 px-2 py-1 rounded-full font-mono">
                  {currentSessionId.slice(-8)}...
                </span>
              </div>
            </div>
            
            {conversationHistory.length === 0 ? (
              <div className="text-center py-12">
                <div className="w-16 h-16 mx-auto mb-4 bg-gradient-to-br from-indigo-100 to-purple-100 rounded-full flex items-center justify-center">
                  <MessageSquare className="w-8 h-8 text-indigo-600" />
                </div>
                <p className="text-gray-500 text-lg mb-2">Start a conversation</p>
                <p className="text-gray-400 text-sm">Your messages will appear here</p>
              </div>
            ) : (
              <div className="space-y-6 max-h-96 overflow-y-auto pr-2 custom-scrollbar">
                {conversationHistory.map((turn, index) => (
                  <div key={index} className="space-y-3">
                    {/* User Message */}
                    <div className="flex justify-end">
                      <div className="group max-w-[75%]">
                        <div className="bg-gradient-to-br from-blue-500 to-blue-600 text-white rounded-2xl rounded-br-md px-4 py-3 shadow-lg">
                          <p className="text-sm leading-relaxed">{turn.user}</p>
                        </div>
                        <div className="flex items-center justify-end mt-1 space-x-2 opacity-0 group-hover:opacity-100 transition-opacity">
                          <span className="text-xs text-gray-400">
                            {turn.timestamp.toLocaleTimeString([], { 
                              hour: '2-digit', 
                              minute: '2-digit' 
                            })}
                          </span>
                          {turn.user_audio_path && (
                            <div className="flex items-center gap-1">
                              {playingAudio === turn.user_audio_path ? (
                                <Button
                                  size="sm"
                                  variant="secondary"
                                  onClick={stopAudio}
                                  className="h-6 px-2"
                                >
                                  <Square className="w-3 h-3" />
                                </Button>
                              ) : (
                                <Button
                                  size="sm"
                                  variant="secondary"
                                  onClick={() => playRecording(turn.user_audio_path!, 'user')}
                                  className="h-6 px-2"
                                >
                                  <PlayCircle className="w-3 h-3" />
                                </Button>
                              )}
                              <Button
                                size="sm"
                                variant="ghost"
                                onClick={() => downloadAudio(turn.user_audio_path!)}
                                className="h-6 px-2"
                                title="Download user audio"
                              >
                                <Download className="w-3 h-3" />
                              </Button>
                            </div>
                          )}
                          <User className="w-3 h-3 text-gray-400" />
                        </div>
                      </div>
                    </div>
                    
                    {/* AI Response */}
                    <div className="flex justify-start">
                      <div className="group max-w-[75%]">
                        <div className="flex items-start space-x-3">
                          <div className="flex-shrink-0">
                            <div className="w-8 h-8 rounded-full overflow-hidden border-2 border-white shadow-md">
                              <Image
                                src={currentAssistant.image}
                                alt={currentAssistant.name}
                                width={32}
                                height={32}
                                className="w-full h-full object-cover"
                              />
                            </div>
                          </div>
                          <div className="flex-1">
                            <div className="bg-white border border-gray-200 rounded-2xl rounded-bl-md px-4 py-3 shadow-lg">
                              <div className="flex items-center mb-1">
                                <span className="text-xs font-medium text-gray-600">
                                  {currentAssistant.name}
                                </span>
                              </div>
                              <p className="text-sm text-gray-800 leading-relaxed">{turn.ai}</p>
                            </div>
                            <div className="flex items-center mt-1 space-x-2 opacity-0 group-hover:opacity-100 transition-opacity">
                              <span className="text-xs text-gray-400">
                                {turn.timestamp.toLocaleTimeString([], { 
                                  hour: '2-digit', 
                                  minute: '2-digit' 
                                })}
                              </span>
                              {turn.ai_audio_path && (
                                <div className="flex items-center gap-1">
                                  {playingAudio === turn.ai_audio_path ? (
                                    <Button
                                      size="sm"
                                      variant="secondary"
                                      onClick={stopAudio}
                                      className="h-6 px-2"
                                    >
                                      <Square className="w-3 h-3" />
                                    </Button>
                                  ) : (
                                    <Button
                                      size="sm"
                                      variant="secondary"
                                      onClick={() => playRecording(turn.ai_audio_path!, 'ai')}
                                      className="h-6 px-2"
                                    >
                                      <Volume2 className="w-3 h-3" />
                                    </Button>
                                  )}
                                  <Button
                                    size="sm"
                                    variant="ghost"
                                    onClick={() => downloadAudio(turn.ai_audio_path!)}
                                    className="h-6 px-2"
                                    title="Download AI audio"
                                  >
                                    <Download className="w-3 h-3" />
                                  </Button>
                                </div>
                              )}
                              <div className="w-2 h-2 bg-green-400 rounded-full"></div>
                              <span className="text-xs text-gray-400">AI Assistant</span>
                            </div>
                          </div>
                        </div>
                      </div>
                    </div>
                  </div>
                ))}
              </div>
            )}
          </div>
        </div>

        {/* Current Interaction Bubble (if active) */}
        {(transcription || (aiResponse && conversationHistory.length === 0)) && (
          <div className="max-w-4xl mx-auto mb-6">
            <div className="bg-gradient-to-r from-indigo-50 to-purple-50 backdrop-blur-md rounded-2xl border border-indigo-200 shadow-xl p-6">
              <div className="flex items-center mb-4">
                <div className="w-3 h-3 bg-indigo-500 rounded-full animate-pulse mr-2"></div>
                <span className="text-sm font-medium text-indigo-700">Current Interaction</span>
              </div>
              
              {transcription && (
                <div className="space-y-3 mb-4">
                  <div className="flex justify-end">
                    <div className="bg-gradient-to-br from-blue-500 to-blue-600 text-white rounded-2xl rounded-br-md px-4 py-3 shadow-lg max-w-[75%]">
                      <p className="text-sm leading-relaxed italic">"{transcription}"</p>
                    </div>
                  </div>
                </div>
              )}

              {aiResponse && conversationHistory.length === 0 && (
                <div className="space-y-3">
                  <div className="flex justify-start">
                    <div className="flex items-start space-x-3 max-w-[75%]">
                      <div className="w-8 h-8 rounded-full overflow-hidden border-2 border-white shadow-md">
                        <Image
                          src={currentAssistant.image}
                          alt={currentAssistant.name}
                          width={32}
                          height={32}
                          className="w-full h-full object-cover"
                        />
                      </div>
                      <div className="bg-white border border-gray-200 rounded-2xl rounded-bl-md px-4 py-3 shadow-lg">
                        <div className="flex items-center mb-1">
                          <span className="text-xs font-medium text-gray-600">
                            {currentAssistant.name}
                          </span>
                        </div>
                        <p className="text-sm text-gray-800 leading-relaxed">{aiResponse}</p>
                      </div>
                    </div>
                  </div>
                </div>
              )}
            </div>
          </div>
        )}

        {/* Status Display */}
        <div className="text-center text-lg font-medium text-gray-700">
          {isRecording && "üé§ Listening..."}
          {isProcessing && "ü§î Thinking..."}
          {isPlaying && "üîä Speaking..."}
          {!isRecording && !isProcessing && !isPlaying && (
            conversationHistory.length === 0 ? "Ready to start conversation" : "Ready for next message"
          )}
        </div>
      </div>

      {/* Custom Scrollbar Styles */}
      <style jsx>{`
        .custom-scrollbar::-webkit-scrollbar {
          width: 6px;
        }
        
        .custom-scrollbar::-webkit-scrollbar-track {
          background: #f1f5f9;
          border-radius: 3px;
        }
        
        .custom-scrollbar::-webkit-scrollbar-thumb {
          background: #cbd5e1;
          border-radius: 3px;
        }
        
        .custom-scrollbar::-webkit-scrollbar-thumb:hover {
          background: #94a3b8;
        }
      `}</style>
    </div>
  )
}
